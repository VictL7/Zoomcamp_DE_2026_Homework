# ğŸ“˜ Zoomcamp_DE_2026_Homework

This repository contains my homework for the **Data Engineering Zoomcamp 2026** by DataTalksClub.  


- **1.1 Docker Data Ingestion** â€“ building a containerized data pipeline  
- **1.2 Terraform & GCP** â€“ provisioning cloud infrastructure for data storage and processing
- **2. Kestra Workflow Orchestration** â€“ parameterized ingestion pipelines, backfill, scheduling, and a simple RAG workflow  
- **3. BigQuery Data Warehousing** - mastering columnar storage, Partitioning, Clustering, and cost-effective data strategies.


---

# ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ 1.1_Docker_Data_ingestion_SQL/      # Docker + Data Ingestion pipeline
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ Data_ingest.py
â”‚   â”‚   â”œâ”€â”€ Data_ingest.ipynb
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ DOCKER_README.md
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 1.2_Terraform_GCP/                  # Terraform + GCP environment
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 2_Kestra_work_orchestration/        # Kestra workflows (Week 2)
â”‚ â”œâ”€â”€ 01_gcp_kv_taxi.yaml               # Store GCP config in KV
â”‚ â”œâ”€â”€ 02_gcp_setup_taxi.yaml            # Create GCS bucket & BigQuery dataset
â”‚ â”œâ”€â”€ 03_gcp_taxi_ingest.yaml           # Parameterized ingestion of taxi CSVs
â”‚ â”œâ”€â”€ 04_gcp_taxi_scheduled_backfill.yaml # Backfill & scheduled workflows
â”‚ â”œâ”€â”€ 05_chat_with_rag.yaml             # RAG workflow using Zoomcamp docs
â”‚ â”œâ”€â”€ Docker_Compose.yaml               # Local Kestra + Postgres + pgAdmin setup
â”‚ â””â”€â”€ README.md                         # Detailed instructions for Kestra flows
â”‚
â”œâ”€â”€ 3_data_warehouse_Bigquery/          # BigQuery Warehouse (Week 3)
â”‚   â”œâ”€â”€ Bigquery_hw3.sql                # SQL queries for optimization & analysis
â”‚   â”œâ”€â”€ load_yellow_taxi_data.py        # Python script to load Parquet to GCS
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â””â”€â”€ README.md                       # Week 3 specific documentation
â”‚
â””â”€â”€ README.md                           # This file
```

---

# ğŸš€ 1.1 Docker Data Ingestion

This module implements an end-to-end pipeline for **NYC Green Taxi data**:

- Downloads NYC Green Taxi trip data (Parquet) and Taxi Zone Lookup (CSV)
- Processes data using **Python (pandas / pyarrow)**
- Ingests data into **PostgreSQL** running inside Docker
- Provides SQL access through **pgAdmin**
- Fully containerized using **Docker & Docker Compose**

### ğŸ”‘ Key Learnings

- Container networking and orchestration with Docker Compose  
- Debugging PostgreSQL connection issues  
- Organizing project files and Docker volumes for persistent data  
- Hands-on experience writing SQL queries for analytics  

---

# â˜ï¸ 1.2 Terraform & GCP

This module sets up cloud infrastructure for data pipelines using **Terraform**:

- Provisions resources on **Google Cloud Platform (GCP)**
- Creates a **GCS bucket** for raw data storage
- Creates a **BigQuery dataset** for analytical queries
- Demonstrates **Infrastructure as Code (IaC)** principles

### ğŸ”‘ Key Learnings

- Defining cloud resources using Terraform configuration files  
- Deploying, modifying, and destroying infrastructure programmatically  
- Understanding **BigQuery** as a managed data warehouse  
- Integrating local, VM, and cloud environments for reproducible pipelines  

---
# âš¡ 2.1 Kestra Workflow Orchestration

This module demonstrates **workflow orchestration using Kestra**, including:

- Parameterized ingestion pipelines for **NYC Taxi data**
- Backfill support for historical CSV data
- Scheduled workflows (monthly ingestion)
- Integration with **GCP** (GCS & BigQuery)
- A minimal **RAG (Retrieval-Augmented Generation) workflow** using course documentation

  
### ğŸ”‘ Key Learnings

- Using **Kestra** for orchestration and scheduling
- Parameterizing workflows for **reusable pipelines**
- Backfill vs scheduled execution
- Secure management of **configuration and secrets**
- Integrating **data ingestion** with cloud storage (GCS) and analytics (BigQuery)
- Introduction to **RAG workflows** in a data engineering context

---
## ğŸ“Š 3.1 BigQuery Data Warehousing

This module focuses on building a high-performance **Data Warehouse** using **Google BigQuery** with **2024 NYC Yellow Taxi** data.

- Storage Architectures: Comparison between **External Tables** (GCS-backed) and **Native Tables** (BigQuery-backed)
- **Columnar Storage**: Understanding how BigQuery optimizes scan volume by reading only the required columns
- Query Optimization: Implementing **Partitioning** and **Clustering** to minimize costs and maximize query performance


## ğŸ”‘ Key Learnings

### Columnar Storage Efficiency
- Leveraged BigQueryâ€™s ability to prune columns  
- Avoided `SELECT *` to significantly reduce query costs

### Partitioning Strategy
- Partitioned tables by `tpep_dropoff_datetime` (Day)  
- Enabled BigQuery to skip scanning irrelevant date ranges

### Clustering Strategy
- Clustered data by `VendorID` within partitions  
- Improved performance for sorting and filtering operations

### Best Practices
- Avoid clustering on small tables (< 1GB), where the overhead outweighs performance benefits  
- Native tables provide significantly better performance than external tables due to optimized metadata handling
- Table Size Threshold: Learned that clustering is generally recommended only for **tables > 1 GB** to ensure the performance gain outweighs the metadata management overhead
- **Metadata Optimization**: Observed that Native Tables leverage built-in metadata (like row counts) much faster than External Tables


---

# ğŸ“Œ Notes

This repository is part of my learning journey through the Data Engineering Zoomcamp.  
Each module includes its own README with detailed instructions, code, and explanations.

